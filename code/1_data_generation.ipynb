{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Data generation\n",
    "\n",
    "This notebooks deals with data augmentation as well as generating test and train sets.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "Given the small amount of useable data we've had (572 frames in the latest .h5 file), we've decided to use data augmentation to generate more training data. For doing so, we used the `imgaug` package which provided us with all useful methods.\n",
    "\n",
    "We decided to use augmentations that made sense in the context of our task. Since our network should deal with images of warped, stretched, translated/rotated brains, we decided to apply those augmentations to the images as well as add noise.\n",
    "\n",
    "Using `iaa.SomeOf` and `iap.Choice`, we randomly choose 2 to 4 transformations to be applied on a given image, with any black artifacts due to rotating or translating being filled with a random background value. We used :\n",
    "\n",
    "- `shearXY` for stretching\n",
    "- `Rotate` for rotating\n",
    "- `PiecewiseAffine` for distortion\n",
    "- `Affine` for translation and zoom\n",
    "- `AdditiveGaussianNoise` for adding noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "from imgaug import parameters as iap\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "from util.handle_files import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uniformly sampling the background value for which we use to fill black spots\n",
    "bg = iap.Uniform(16,18)\n",
    "#Creating a series of augmentations\n",
    "shearXY = iaa.Sequential([\n",
    "                           iaa.ShearY(iap.Uniform(-10,10),cval=bg),\n",
    "                           iaa.ShearX(iap.Uniform(-10,10),cval=bg)\n",
    "                       ])\n",
    "rotate = iaa.Rotate(rotate = iap.Choice([-30,-15,15,30],\n",
    "                                        p=[0.25,0.25,0.25,0.25]),\n",
    "                    cval = bg)\n",
    "\n",
    "pwAff = iaa.PiecewiseAffine(scale=(0.01,0.06),\n",
    "                            cval = bg)\n",
    "\n",
    "affine = iaa.Affine(scale={\"x\":iap.Uniform(1.1,1.2),\n",
    "                                       \"y\":iap.Uniform(1.1,1.2)},\n",
    "                    cval = bg)\n",
    "\n",
    "noise = iaa.AdditiveGaussianNoise(loc=0, scale=(0,0.025*255))\n",
    "#Using SomeOf to randomly select some augmentations\n",
    "someAug = iaa.SomeOf(iap.Choice([2,3,4], p = [1/3,1/3,1/3]),\n",
    "                     [\n",
    "                         affine,\n",
    "                         shearXY,\n",
    "                         pwAff,\n",
    "                         rotate,\n",
    "                         noise\n",
    "                     ], random_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images(input_path, augs, times_target=1):\n",
    "    #Load input image\n",
    "    imgs = []\n",
    "    print(\"Loading images from :\", input_path)\n",
    "    for f in glob.iglob(input_path+'*.jpg'):\n",
    "        imgs.append(np.asarray(Image.open(f)))\n",
    "        \n",
    "    #Initialize list to receive augmented target images\n",
    "    #For each time we want to augment (since random augmentation)\n",
    "    target_imgs = []\n",
    "    #Create the directory to receive the images in\n",
    "    target_path = input_path+'/augmented/'\n",
    "    target_path = makedir(target_path)\n",
    "    \n",
    "    #Applying (times_target) times the augmentation\n",
    "    for i in range(times_target):\n",
    "        print(\"Augmenting\", i, \"/\",times_target-1)\n",
    "        target_imgs += augs.augment_images(imgs)\n",
    "        \n",
    "    print(\"Saving images\")\n",
    "    for idx, augmented in enumerate(target_imgs):\n",
    "        img = Image.fromarray(augmented)\n",
    "        img.save(target_path+'augmented_frame_'+str(idx)+'.jpg')\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we decided to only use RGB images for now.\n",
    "For a batch, each time the transformations are selected randomly, using `SomeOf` and `iap.Choice`. This way, even if we augment 10 times with the same set of transformations, the results won't be all the same.\n",
    "\n",
    "In the end, we get a 10-fold augmentation yielding a total of 5720 images (including the originals) for training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datapaths\n",
    "paths = ['../worm_data/h5_data/512/rgb_frames/',\n",
    "         '../worm_data/h5_data/240/rgb_frames/']\n",
    "\n",
    "#Augmenting for all\n",
    "for path in paths:\n",
    "    augment_images(path, someAug, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Train/Validation sets\n",
    "\n",
    "#### This part required a bit of manual handling initially, but generate_trainval.py bypassed this issue by directly extracting data and generating the augmentation as well as csvs in the appropriate folders.\n",
    "\n",
    "For a given training datasets, :\n",
    "- create a folder with an appropriate name in `./datasets/` containing a folder named `TrainVal` and move all images in it.\n",
    "- create a folder with an appropriate name in `./training_data/` suffixed with -random\n",
    "\n",
    "For example, for the **rgb240_augmented** dataset:\n",
    "\n",
    "- create `./datasets/rgb240_augmented/TrainVal` which will contain all the images\n",
    "- create `./training_data/rgb240_augmented-random/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_split(image_dir, csv_dir='', ratio=0.2):\n",
    "    \"\"\"\n",
    "    Create csv files containing the filenames to use in each of the \n",
    "    training and validation set.\n",
    "    \"\"\"\n",
    "    data=[]\n",
    "    csvdir = image_dir.split('datasets/')[1].split('/TrainVal')[0]\n",
    "    csv_dir = makedir('./training_data/'+csvdir+'-random/')\n",
    "    #else if csv_dir != '': \n",
    "    #    csvdir = makedir(csv_dir)\n",
    "    #\n",
    "    with open(csv_dir+'data.csv', 'w', newline='') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(['image'])\n",
    "        for filename in os.listdir(image_dir):\n",
    "            data.append('TrainVal/'+filename)\n",
    "            writer.writerow(data)\n",
    "            data=[]\n",
    "    writeFile.close()\n",
    "    \n",
    "    df = pd.read_csv(csv_dir+'data.csv')\n",
    "    train, val = train_test_split(df, test_size=ratio)\n",
    "    \n",
    "    train.to_csv(csv_dir+'train.csv', index=False)\n",
    "    val.to_csv(csv_dir+'val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdr = ['./datasets/rgb240_augmented/TrainVal', \n",
    "         './datasets/red240_augmented/TrainVal',\n",
    "         './datasets/rgb512_augmented/TrainVal']\n",
    "\n",
    "csvdr = ['./training_data/rgb240_augmented-random/',\n",
    "         '', #use empty str here to test my method\n",
    "         './training_data/rgb512_augmented-random/']\n",
    "\n",
    "for x,y in zip(imgdr, csvdr):\n",
    "    create_csv_split(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting dataset means and stds to be used for standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import numpy as np\n",
    "\n",
    "x = list_files('./datasets/rgb512_augmented/TrainVal/', 'jpg')\n",
    "imgs = []\n",
    "for f in x :\n",
    "    imgs.append(io.imread(f))\n",
    "arr = np.asarray(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red 0.0742920482408156 0.01979817471748977\n",
      "green 0.0754988783485235 0.03203817782416048\n"
     ]
    }
   ],
   "source": [
    "print(\"red\",arr[...,0].mean()/255, arr[...,0].std()/255)\n",
    "print(\"green\", arr[...,1].mean()/255, arr[...,1].std()/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue 0.00703948247409379 0.015462965792484733\n"
     ]
    }
   ],
   "source": [
    "print(\"blue\", arr[...,2].mean()/255, arr[...,2].std()/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a previous notebook, the datasets were found to have the following means and stds:\n",
    "```\n",
    "means = [0.0743, 0.0755, 0.0070]\n",
    "stds = [0.0198, 0.0320, 0.0155]\n",
    "```\n",
    "These values will be used for standardization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
