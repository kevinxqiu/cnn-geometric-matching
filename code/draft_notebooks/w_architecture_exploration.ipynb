{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../../code\")\n",
    "from model.cnn_geometric_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using their preset network to figure out dimensions\n",
    "affine = CNNGeometric(output_dim=6, use_cuda=False)\n",
    "tps = CNNGeometric(output_dim=18, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method Module.children of CNNGeometric(\n",
       "  (FeatureExtraction): FeatureExtraction(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (FeatureCorrelation): FeatureCorrelation(\n",
       "    (ReLU): ReLU()\n",
       "  )\n",
       "  (FeatureRegression): FeatureRegression(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(225, 128, kernel_size=(7, 7), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (linear): Linear(in_features=1600, out_features=6, bias=True)\n",
       "  )\n",
       "  (ReLU): ReLU(inplace=True)\n",
       ")>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#Getting children\n",
    "affine.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input shape:\n torch.Size([1, 3, 240, 240])\nAfter Feat.Extract,\n torch.Size([1, 512, 15, 15])\nAfter Feat.Corr,\n torch.Size([1, 225, 15, 15])\nAfter Feat Reg Conv,\n torch.Size([1, 64, 5, 5])\nAfter Feat Reg Lin,\n torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty((1,3,240,240))\n",
    "x1 = affine.FeatureExtraction(x)\n",
    "x2 = affine.FeatureCorrelation(x1,x1)\n",
    "\n",
    "#FeatureRegression has a Conv and Linear part. \n",
    "x3 = affine.FeatureRegression.conv(x2)\n",
    "x4 = affine.FeatureRegression.linear(x3.reshape(1,-1)) #Need to reshape x3 to make it fit into the linear.\n",
    "\n",
    "print(\"Input shape:\\n\", x.shape)\n",
    "print(\"After Feat.Extract,\\n\", x1.shape)\n",
    "print(\"After Feat.Corr,\\n\", x2.shape)\n",
    "print(\"After Feat Reg Conv,\\n\", x3.shape)\n",
    "print(\"After Feat Reg Lin,\\n\", x4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pedro's model\n",
    "pedro_fe = nn.Sequential(\n",
    "    nn.Conv2d(3,128,3,padding=1), #Better using little kernels if we want to detect small objects. Also use odd numbers \n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128,256,3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "\n",
    "    nn.Conv2d(256,512,3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    nn.Conv2d(512,512,5,padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    nn.Conv2d(512,512,5,padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): LeakyReLU(negative_slope=0.01)\n",
      "  (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): ReLU()\n",
      "  (7): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (9): LeakyReLU(negative_slope=0.01)\n",
      "  (10): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (12): ReLU()\n",
      "  (13): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (15): LeakyReLU(negative_slope=0.01)\n",
      "  (16): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (17): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (18): ReLU()\n",
      ")\n",
      "torch.Size([1, 512, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "#Richie's model\n",
    "x = torch.empty((1,3,240,240),device='cuda')\n",
    "def bn2d(x): return nn.BatchNorm2d(x)\n",
    "leaky = nn.LeakyReLU()\n",
    "fe = nn.Sequential(nn.Conv2d(3,32, kernel_size = (5,5)),\n",
    "                   bn2d(32),\n",
    "                   leaky,\n",
    "                   nn.Conv2d(32,64, kernel_size = (5,5)),\n",
    "                   bn2d(64),\n",
    "                   nn.MaxPool2d(kernel_size=2),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64,128, kernel_size = (5,5)),\n",
    "                   bn2d(128),\n",
    "                   leaky,\n",
    "                   nn.Conv2d(128,256, kernel_size = (5,5)),\n",
    "                   nn.MaxPool2d(kernel_size=2),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(256,512, kernel_size = (5,5)),\n",
    "                   bn2d(512),\n",
    "                   leaky,\n",
    "                   nn.Conv2d(512,512, kernel_size = (5,5)),\n",
    "                   nn.MaxPool2d(kernel_size=3),\n",
    "                   nn.ReLU())\n",
    "fe.cuda()\n",
    "print(fe)\n",
    "print(fe(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python -W ignore train.py --training-dataset rgb512_aug --image-size 512 --feature-extraction-cnn resnet101 --geometric-model affine --pretrained False --trained-model-dir ./trained_models/ --trained-model-fn baseline_resnet_affine --num-epochs 40 --lr 0.00678  --momentum 0.915\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `wormbrain_1`\n",
    "\n",
    "##### Affine\n",
    "```\n",
    "python -W ignore train.py --training-dataset rgb512_aug --geometric-model affine --feature-extraction-cnn wormbrain_1 --trained-model-dir ./trained_models/test_arch --trained-model-fn new_std --num-epochs 35 --lr 0.000578 --image-size 512 --momentum 0.93\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TPS\n",
    "```\n",
    "python -W ignore train.py --training-dataset rgb512_aug --geometric-model tps --feature-extraction-cnn wormbrain_1 --trained-model-dir ./trained_models/test_arch --trained-model-fn TPS_hyperparms --num-epochs 35 --lr 0.000878 --image-size 512 --momentum 0.93 --weight-decay 0.0009\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.handle_files import *\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "                               \n",
    "x = list_files('../datasets/rgb512_augmented/TrainVal/', 'jpg')\n",
    "imgs = []             \n",
    "for f in x :\n",
    "    imgs.append(io.imread(f))\n",
    "arr = np.asarray(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "red 0.0742920482408156 0.01979817471748977\n",
      "green 0.0754988783485235 0.03203817782416048\n"
     ]
    }
   ],
   "source": [
    "print(\"red\",arr[...,0].mean()/255, arr[...,0].std()/255)\n",
    "print(\"green\", arr[...,1].mean()/255, arr[...,1].std()/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "blue 0.00703948247409379 0.015462965792484733\n"
     ]
    }
   ],
   "source": [
    "print(\"blue\", arr[...,2].mean()/255, arr[...,2].std()/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHANGING THE FEATURE REGRESSOR\n",
    "\n",
    "I've found the feature regressor to be quite (too?) simple, with no operations.\n",
    "I decided to add a small MLP for feature regression prediction, see below\n",
    "\n",
    "```\n",
    "self.linear = nn.Linear(ch_out * kernel_sizes[-1] * kernel_sizes[-1], output_dim)\n",
    "```\n",
    "\n",
    "is changed into : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "self.Dropout = nn.Dropout(0.25)\n",
    "self.linear = nn.Sequential(\n",
    "                nn.Linear(ch_out * kernel_sizes[-1] * kernel_sizes[-1], 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(1024),\n",
    "                self.Dropout,\n",
    "                nn.Linear(1024,512),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(512),\n",
    "                self.Dropout,\n",
    "                nn.Linear(512,256),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(256),\n",
    "                self.Dropout,\n",
    "                nn.Linear(256,128),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(128),\n",
    "                self.Dropout,\n",
    "                nn.Linear(128,64),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(64),\n",
    "                self.Dropout,\n",
    "                nn.Linear(64, output_dim))\n",
    "```\n",
    "\n",
    "This helped me regularize training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Training parameters used for this new Feat. Reg. and architecture \"wormbrain_2\" (to use ELU instead of LeakyReLU)\n",
    "\n",
    "Using a smaller training set to be quicker (3000 images vs 5720).\n",
    "\n",
    "#### AFFINE\n",
    "```\n",
    "python -W ignore train.py --training-dataset smallset --geometric-model affine --feature-extraction-cnn wormbrain_2 --trained-model-dir ./trained_models/test_arch --trained-model-fn new_std --num-epochs 35 --lr 0.001234 --image-size 512 --momentum 0.91\n",
    "```\n",
    "```\n",
    "python -W ignore train.py --training-dataset rgb512_aug --geometric-model affine --feature-extraction-cnn wormbrain_1 --trained-model-dir ./trained_models/test_arch/wormbrain_1/ --trained-model-fn new_featreg_relu --num-epochs 40 --lr 0.02667 --image-size 512 --momentum 0.9\n",
    "```\n",
    "```\n",
    "python -W ignore train.py --training-dataset rgb512_aug --geometric-model affine --feature-extraction-cnn wormbrain_1 --trained-model-dir ./trained_models/test_arch/wormbrain_2/ --trained-model-fn new_featreg_relu --num-epochs 40 --lr 0.01667 --image-size 512 --momentum 0.95\n",
    "```\n",
    "```\n",
    "python -W ignore train.py --training-dataset smallset --geometric-model affine --feature-extraction-cnn wormbrain_1 --trained-model-dir ./trained_models/test_arch/smallset/ --trained-model-fn t1_new_featreg_relu --num-epochs 40 --lr 0.001667 --image-size 512 --momentum 0.92\n",
    "```\n",
    "```\n",
    "python -W ignore train.py --training-dataset smallset --geometric-model tps --feature-extraction-cnn wormbrain_1 --trained-model-dir ./trained_models/test_arch/smallset/ --trained-model-fn wormbrain_1 --num-epochs 40 --lr 0.003667 --image-size 512 --momentum 0.915 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to train the new regressor + Richie's architecture using the full dataset : \n",
    "These are the \"best\" overall I got.\n",
    "\n",
    "I haven't logged the results for the other as they were either unstable \n",
    "(loss oscillate between epochs) or didn't converge to a low enough value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### **Affine** : \n",
    "\n",
    "``` \n",
    "python -W ignore train.py --training-dataset rgb512_aug --geometric-model affine --feature-extraction-cnn wormbrain_1 --trained-model-dir ./trained_models/test_arch/fullset/ --trained-model-fn wormbrain_1_newFR_dropout --num-epochs 40 --lr 0.0023667 --image-size 512 --momentum 0.92 \n",
    "```\n",
    "With Dropout set to 0.25, using wormbrain_1 (LeakyReLU instead of ELU)\n",
    "    \n",
    "Final epoch (40) : Train loss = 0.280 , Validation loss = 0.252** \n",
    "\n",
    "BEST epoch (35) : Train loss = 0.262, Validation loss = 0.197**\n",
    "\n",
    "-------------------------\n",
    "```\n",
    "python -W ignore train.py --training-dataset rgb512_aug --geometric-model affine --feature-extraction-cnn wormbrain_1 --feature-regression wormbrain --fr-dropout 0.2 --trained-model-dir ./trained_models/test_arch/fullset/ --trained-model-fn AffineWB_dropout02 --num-epochs 50 --lr 0.004667 --image-size 512 --momentum 0.92 --lr_max_iter 10000\n",
    "```\n",
    "\n",
    "With Dropout set to 0.2, wb1\n",
    "\n",
    "**Final epoch (50) : Train loss = 0.293 , Validation loss = 0.281** \n",
    "\n",
    "**BEST epoch (36) : Train loss = 0.269, Validation loss = 0.204**\n",
    "\n",
    "-----------------------\n",
    "#### **Thin Plate Spline**\n",
    "```\n",
    "python -W ignore train.py --training-dataset rgb512_aug --geometric-model tps --feature-extraction-cnn wormbrain_2 --trained-model-dir ./trained_models/test_arch/fullset/ --trained-model-fn TPS_wormbrain_2_newFR_dropout015xd --num-epochs 50 --lr 0.003667 --image-size 512 --momentum 0.94\n",
    "```\n",
    " \n",
    "With dropout = 0.2, using wormbrain_2 (ELU instead of LeakyReLU)\n",
    "\n",
    "    **Final result : Train loss = 0.411 , Validation loss = 0.397** \n",
    "    \n",
    "    **BEST epoch (47) : Train loss = 0.406, Validation loss = 0.378**\n",
    "\n",
    "------------------------\n",
    "\n",
    "    \n",
    "```\n",
    "python -W ignore train.py --training-dataset rgb512_aug --geometric-model tps --feature-extraction-cnn wormbrain_2 --feature-regression wormbrain --fr-dropout 0.35 --trained-model-dir ./trained_models/test_arch/fullset/ --trained-model-fn tpsWB_dropout035 --num-epochs 50 --lr 0.005667 --image-size 512 --momentum 0.935 --lr_max_iter 7500\n",
    "```\n",
    "\n",
    "With dropout = 0.35, wb2\n",
    "training set : Average loss: 0.0465\n",
    "Validation set: Average loss: 0.0464\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpler Feature Regressor\n",
    "\n",
    "Having tried to train the feature regressor with TPS many times with no success (doesn't go much below 0.379 validation loss), I've decided to reduce the complexity of the model, as well as adding a softmax activation in the early layer to normalize the features to (0,1), i.e. a probability distribution, and have the network act like a multinomial logistic regression.\n",
    "\n",
    "Thus, the model is reduced from :\n",
    "```\n",
    "self.Dropout = nn.Dropout(0.25)\n",
    "self.linear = nn.Sequential(\n",
    "                nn.Linear(ch_out * kernel_sizes[-1] * kernel_sizes[-1], 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(1024),\n",
    "                self.Dropout,\n",
    "                nn.Linear(1024,512),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(512),\n",
    "                self.Dropout,\n",
    "                nn.Linear(512,256),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(256),\n",
    "                self.Dropout,\n",
    "                nn.Linear(256,128),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(128),\n",
    "                self.Dropout,\n",
    "                nn.Linear(128,64),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(64),\n",
    "                self.Dropout,\n",
    "                nn.Linear(64, output_dim))\n",
    "```\n",
    "\n",
    "to \n",
    "\n",
    "```\n",
    "self.Dropout = nn.Dropout(p_dropout)\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(ch_out * kernel_sizes[-1] * kernel_sizes[-1], 512),\n",
    "                nn.BatchNorm1d(512), \n",
    "                nn.Softmax(dim=1),\n",
    "                self.Dropout,\n",
    "                \n",
    "                nn.Linear(512,128),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(128),\n",
    "                self.Dropout,\n",
    "                \n",
    "                nn.Linear(128,output_dim))\n",
    "```\n",
    "\n",
    "This allowed the best average validation loss to reach around 0.0304 (versus 0.5 before), as well as converging earlier without re-increasing. (~20 epochs, versus 30-35 epochs before) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4aa635535829e595f413464b8d4b52eacf31c3326ae9850a263ad3e53fc82e1d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}