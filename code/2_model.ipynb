{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "See hidden cell below for the set of options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] [--feature-extraction-cnn FEATURE_EXTRACTION_CNN]\n",
      "                [--pretrained PRETRAINED]\n",
      "                [--feature-extraction-last-layer FEATURE_EXTRACTION_LAST_LAYER]\n",
      "                [--fr-kernel-sizes FR_KERNEL_SIZES [FR_KERNEL_SIZES ...]]\n",
      "                [--fr-channels FR_CHANNELS [FR_CHANNELS ...]]\n",
      "                [--matching-type MATCHING_TYPE]\n",
      "                [--normalize-matches [NORMALIZE_MATCHES]] [--lr LR]\n",
      "                [--lr_scheduler [LR_SCHEDULER]] [--lr_max_iter LR_MAX_ITER]\n",
      "                [--momentum MOMENTUM] [--num-epochs NUM_EPOCHS]\n",
      "                [--batch-size BATCH_SIZE] [--weight-decay WEIGHT_DECAY]\n",
      "                [--seed SEED] [--use-mse-loss [USE_MSE_LOSS]]\n",
      "                [--geometric-model GEOMETRIC_MODEL]\n",
      "                [--trained-model-fn TRAINED_MODEL_FN]\n",
      "                [--trained-model-dir TRAINED_MODEL_DIR]\n",
      "                [--training-dataset TRAINING_DATASET]\n",
      "                [--train-dataset-size TRAIN_DATASET_SIZE]\n",
      "                [--test-dataset-size TEST_DATASET_SIZE]\n",
      "                [--train-fe [TRAIN_FE]] [--train-fr [TRAIN_FR]]\n",
      "                [--train-bn [TRAIN_BN]]\n",
      "                [--fe-finetune-params FE_FINETUNE_PARAMS [FE_FINETUNE_PARAMS ...]]\n",
      "                [--update-bn-buffers [UPDATE_BN_BUFFERS]]\n",
      "                [--occlusion-factor OCCLUSION_FACTOR]\n",
      "                [--log_interval LOG_INTERVAL] [--log_dir LOG_DIR]\n",
      "                [--dataset-csv-path DATASET_CSV_PATH]\n",
      "                [--dataset-image-path DATASET_IMAGE_PATH]\n",
      "                [--four-point-hom [FOUR_POINT_HOM]]\n",
      "                [--random-sample [RANDOM_SAMPLE]] [--random-t RANDOM_T]\n",
      "                [--random-s RANDOM_S] [--random-alpha RANDOM_ALPHA]\n",
      "                [--random-t-tps RANDOM_T_TPS] [--image-size IMAGE_SIZE]\n",
      "                [--model MODEL] [--num-workers NUM_WORKERS]\n",
      "\n",
      "CNNGeometric PyTorch implementation\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "model:\n",
      "  --feature-extraction-cnn FEATURE_EXTRACTION_CNN\n",
      "                        feature extraction CNN model architecture:\n",
      "                        vgg/resnet101\n",
      "  --pretrained PRETRAINED\n",
      "                        load pre-trained model from Pytorch : False/True\n",
      "  --feature-extraction-last-layer FEATURE_EXTRACTION_LAST_LAYER\n",
      "                        feature extraction CNN last layer\n",
      "  --fr-kernel-sizes FR_KERNEL_SIZES [FR_KERNEL_SIZES ...]\n",
      "                        kernels sizes in feat.reg. conv layers\n",
      "  --fr-channels FR_CHANNELS [FR_CHANNELS ...]\n",
      "                        channels in feat. reg. conv layers\n",
      "  --matching-type MATCHING_TYPE\n",
      "                        correlation/subtraction/concatenation\n",
      "  --normalize-matches [NORMALIZE_MATCHES]\n",
      "                        perform L2 normalization\n",
      "\n",
      "train:\n",
      "  --lr LR               learning rate\n",
      "  --lr_scheduler [LR_SCHEDULER]\n",
      "                        Bool (default True), whether to use a decaying\n",
      "                        lr_scheduler\n",
      "  --lr_max_iter LR_MAX_ITER\n",
      "                        Number of steps between lr starting value and 1e-6 (lr\n",
      "                        default min) when choosing lr_scheduler\n",
      "  --momentum MOMENTUM   momentum constant\n",
      "  --num-epochs NUM_EPOCHS\n",
      "                        number of training epochs\n",
      "  --batch-size BATCH_SIZE\n",
      "                        training batch size\n",
      "  --weight-decay WEIGHT_DECAY\n",
      "                        weight decay constant\n",
      "  --seed SEED           Pseudo-RNG seed\n",
      "  --use-mse-loss [USE_MSE_LOSS]\n",
      "                        Use MSE loss on tnf. parameters\n",
      "  --geometric-model GEOMETRIC_MODEL\n",
      "                        affine/hom/tps\n",
      "  --trained-model-fn TRAINED_MODEL_FN\n",
      "                        trained model filename\n",
      "  --trained-model-dir TRAINED_MODEL_DIR\n",
      "                        path to trained models folder\n",
      "  --training-dataset TRAINING_DATASET\n",
      "                        dataset to use for training\n",
      "  --train-dataset-size TRAIN_DATASET_SIZE\n",
      "                        train dataset size limit\n",
      "  --test-dataset-size TEST_DATASET_SIZE\n",
      "                        test dataset size limit\n",
      "  --train-fe [TRAIN_FE]\n",
      "                        Train feature extraction\n",
      "  --train-fr [TRAIN_FR]\n",
      "                        Train feature regressor\n",
      "  --train-bn [TRAIN_BN]\n",
      "                        train batch-norm layers\n",
      "  --fe-finetune-params FE_FINETUNE_PARAMS [FE_FINETUNE_PARAMS ...]\n",
      "                        String indicating the F.Ext params to finetune\n",
      "  --update-bn-buffers [UPDATE_BN_BUFFERS]\n",
      "                        Update batch norm running mean and std\n",
      "  --occlusion-factor OCCLUSION_FACTOR\n",
      "                        occlusion factor for training\n",
      "  --log_interval LOG_INTERVAL\n",
      "                        Number of iterations between logs\n",
      "  --log_dir LOG_DIR     If unspecified log_dir will be set\n",
      "                        to<trained_models_dir>/<trained_models_fn>/\n",
      "\n",
      "dataset:\n",
      "  --dataset-csv-path DATASET_CSV_PATH\n",
      "                        path to training transformation csv folder\n",
      "  --dataset-image-path DATASET_IMAGE_PATH\n",
      "                        path to folder containing training images\n",
      "  --four-point-hom [FOUR_POINT_HOM]\n",
      "                        use 4 pt parametrization for homography\n",
      "  --random-sample [RANDOM_SAMPLE]\n",
      "                        sample random transformations\n",
      "  --random-t RANDOM_T   random transformation translation\n",
      "  --random-s RANDOM_S   random transformation translation\n",
      "  --random-alpha RANDOM_ALPHA\n",
      "                        random transformation translation\n",
      "  --random-t-tps RANDOM_T_TPS\n",
      "                        random transformation translation\n",
      "\n",
      "base:\n",
      "  --image-size IMAGE_SIZE\n",
      "                        image input size\n",
      "  --model MODEL         Pre-trained model filename\n",
      "  --num-workers NUM_WORKERS\n",
      "                        number of workers\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore train.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model on the augmented 512x512 RGB images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the model and loading weights.\n",
    "\n",
    "After training, we have a sets of trained weights from PyTorch. This notebook shows how to load and use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model.cnn_geometric_model import CNNGeometric\n",
    "from data.pf_dataset import PFDataset\n",
    "from data.download_datasets import download_PF_willow\n",
    "from image.normalization import NormalizeImageDict, normalize_image\n",
    "from util.torch_util import BatchTensorToVars, str_to_bool\n",
    "from geotnf.transformation import GeometricTnf\n",
    "from geotnf.point_tnf import *\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import warnings\n",
    "from torchvision.transforms import Normalize\n",
    "from collections import OrderedDict\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from util.handle_files import * #for list_files(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(aff_params_path = '', aff_feat_ext = 'wormbrain_1', aff_feat_reg ='simpler',\n",
    "               tps_params_path = '', tps_feat_ext = 'wormbrain_1', tps_feat_reg ='simpler'):\n",
    "    \"\"\"\n",
    "    Loads a model. Assumes that each model (Affine and Thin-Plate Spline)\n",
    "    have been trained separately. Must specify the architecture used for feature_extraction\n",
    "    By default, it is resnet101\n",
    "    \"\"\"\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    #Only create a model for which weights have been provided\n",
    "    do_aff = not aff_params_path==''\n",
    "    do_tps = not tps_params_path==''\n",
    "    if not do_aff and not do_tps: \n",
    "        print(\"No weights found. Models not created, exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Creating CNN model.\")\n",
    "    if do_aff:\n",
    "        model_aff = CNNGeometric(output_dim=6,use_cuda=use_cuda,\n",
    "                             feature_extraction_cnn= aff_feat_ext,feature_regression = 'simpler')\n",
    "    if do_tps:\n",
    "        model_tps = CNNGeometric(output_dim=18,use_cuda=use_cuda,\n",
    "                             feature_extraction_cnn= tps_feat_ext,feature_regression = 'simpler')\n",
    "    print(\"Loading trained model weights.\")\n",
    "    \n",
    "    if do_aff: #Loading affine model    \n",
    "        if aff_feat_ext == 'resnet101': aff_feat_ext = 'resnet'\n",
    "            \n",
    "        checkpoint = torch.load(aff_params_path, map_location=lambda storage, loc: storage)\n",
    "        checkpoint['state_dict'] = OrderedDict([(k.replace(aff_feat_ext, 'model'), v) for k, v in checkpoint['state_dict'].items()])\n",
    "        model_aff.load_state_dict(checkpoint['state_dict'])\n",
    "        print('Weights for Affine model loaded.')\n",
    "    \n",
    "    if do_tps: #Loading thin plate spline model\n",
    "        if tps_feat_ext == 'resnet101': aff_feat_ext = 'resnet'\n",
    "    \n",
    "        checkpoint = torch.load(tps_params_path, map_location=lambda storage, loc: storage)\n",
    "        checkpoint['state_dict'] = OrderedDict([(k.replace(aff_feat_ext, 'model'), v) for k, v in checkpoint['state_dict'].items()])\n",
    "        model_tps.load_state_dict(checkpoint['state_dict'])\n",
    "        print('Weights for Thin-Plate Spline model loaded.')\n",
    "    \n",
    "    print('Returning model(s).')\n",
    "    \n",
    "    if do_aff and not do_tps:\n",
    "        return model_aff\n",
    "    if do_tps and not do_aff:\n",
    "        return model_tps\n",
    "    if do_aff and do_tps:\n",
    "        return model_aff, model_tps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the geometric transformation methods\n",
    "use_cuda = torch.cuda.is_available()\n",
    "tpsTnf = GeometricTnf(geometric_model='tps', use_cuda=use_cuda)\n",
    "affTnf = GeometricTnf(geometric_model='affine', use_cuda=use_cuda)\n",
    "#Here resizes to 240x240 with bi-linear sampling\n",
    "resizeCNN = GeometricTnf(out_h=240, out_w=240, use_cuda = False) \n",
    "\n",
    "#Here resizes to 240x240 with bi-linear sampling\n",
    "means = [0.0743, 0.0755, 0.0070]\n",
    "stds = [0.0198, 0.0320, 0.0155]\n",
    "def preprocess_image(image,means,stds):\n",
    "    \"\"\"\n",
    "    Preprocesses the image for warping\n",
    "    \"\"\"\n",
    "    # convert to torch Variable\n",
    "    image = np.expand_dims(image.transpose((2,0,1)),0)\n",
    "    image = torch.Tensor(image.astype(np.float32)/255.0)\n",
    "    image_var = Variable(image,requires_grad=False)\n",
    "    # Resize image using bilinear sampling with identity affine tnf\n",
    "    image_var = resizeCNN(image_var)\n",
    "    # Normalize image\n",
    "    image_var = normalize_image(image_var,mean=means,std=stds)\n",
    "    return image_var\n",
    "\n",
    "normalizeTnf = Normalize(mean = means, std=stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_warped(source_name,target_name, savename, model_aff, model_tps):\n",
    "    \"\"\"\n",
    "    Aligns a source image to a target image, and saves it.\n",
    "    \"\"\"\n",
    "    do_aff, do_tps = not model_aff == '', not model_tps == ''\n",
    "    if not (do_aff and do_tps):\n",
    "        print(\"No model found. Exiting.\")\n",
    "        return\n",
    "    source_image = io.imread(source_name)\n",
    "    target_image = io.imread(target_name)\n",
    "    #Use the preprocess method declared above to resize, \n",
    "    #normalize using the means and stds. Here, by default uses the means/stds of ImageNet,\n",
    "    #Otherwise causes some weird issues we don't understand why.\n",
    "    source_image_var = preprocess_image(source_image,means=means, stds=stds)\n",
    "    target_image_var = preprocess_image(target_image,means=means, stds=stds)\n",
    "    \n",
    "    if use_cuda:\n",
    "        source_image_var = source_image_var.cuda()\n",
    "        target_image_var = target_image_var.cuda()\n",
    "    #Create a \"batch\" (i.e. a pair) for the next cell below\n",
    "    batch = {'source_image': source_image_var, 'target_image':target_image_var}\n",
    "    #Resize target: create a function that will resize a given input into the target_image's dimension\n",
    "    resizeTgt = GeometricTnf(out_h=target_image.shape[0], out_w=target_image.shape[1], use_cuda = use_cuda) \n",
    "    #Set the models to eval mode\n",
    "    if do_aff:\n",
    "        model_aff.eval()\n",
    "    if do_tps:\n",
    "        model_tps.eval()      \n",
    "    # Evaluate models and get the thetas\n",
    "    if do_aff:\n",
    "        theta_aff=model_aff(batch)\n",
    "        warped_image_aff = affTnf(batch['source_image'],theta_aff.view(-1,2,3))\n",
    "    \n",
    "    if do_tps:\n",
    "        theta_tps=model_tps(batch)\n",
    "        warped_image_tps = tpsTnf(batch['source_image'],theta_tps)\n",
    "    \n",
    "    if do_aff and do_tps:\n",
    "        theta_aff_tps=model_tps({'source_image': warped_image_aff, 'target_image': batch['target_image']})        \n",
    "        warped_image_aff_tps = tpsTnf(warped_image_aff,theta_aff_tps)\n",
    "    if do_aff:\n",
    "        warped_image_aff_np = normalize_image(resizeTgt(warped_image_aff),forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "    \n",
    "    if do_tps:\n",
    "        warped_image_tps_np = normalize_image(resizeTgt(warped_image_tps),forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "    \n",
    "    if do_aff and do_tps:\n",
    "        warped_image_aff_tps_np = normalize_image(resizeTgt(warped_image_aff_tps),forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "        x = np.clip(warped_image_aff_tps_np,0,1)\n",
    "    plt.imsave(savename, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment\n",
    "\n",
    "We can either : Choose one or multiple frames to align to (Which ones to choose?)\n",
    "or align to the previous frame. (ex : frame 1710 will be aligned to 1709, etc.)\n",
    "\n",
    "I created this loop on the rationale that if we iterate and try to align to the previous frame multiple times, it will get better at each iteration (i.e. align it \"close to the ground truth\"). In practice, this doesn't work, so iterations is set to 1 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import shutil\n",
    "\n",
    "def batch_align_to_previous(sourcepath, model_aff ='', model_tps ='', iterations=1):\n",
    "    \"\"\"\n",
    "    For a folder of images, align each frame to the previous one.\n",
    "    \"\"\"\n",
    "    input_reversed = list_files(sourcepath, 'jpg',reverse_sort=True)\n",
    "    #getting directories\n",
    "    savepath = makedir(sourcepath+'alignedprevious/')\n",
    "    makedir(savepath+'/iteration_0/')\n",
    "    savepath = sourcepath+'alignedprevious/iteration_0/'\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        #CREATING DIRECTORY TO RECEIVE RESULTS\n",
    "        shutil.copy2(input_reversed[-1], savepath)    \n",
    "        makedir(savepath)\n",
    "        print(\"\\n ### Aligning ###\") \n",
    "        #Looping and doing the warp etc. \n",
    "        for i in range(len(input_reversed)-1):\n",
    "            source = input_reversed[i]\n",
    "            target = input_reversed[i+1]\n",
    "            outname = savepath+'frame_'+str(len(input_reversed)-i-1)+'_to_previous.jpg'\n",
    "            save_warped(source,target,outname, model_aff, model_tps)\n",
    "        \n",
    "        \n",
    "        #Updates the sets for the next iteration\n",
    "        input_reversed = list_files(savepath, 'jpg', reverse_sort = True)        \n",
    "        savepath = savepath.split('iteration')[0]+'iteration_'+str(iteration+1)+'/'\n",
    "    print(\"Done aligning.\")\n",
    "    \n",
    "def save_video(imagepath, reverse = False):\n",
    "    \"\"\"\n",
    "    For a given set of frames, create a video and saves it.\n",
    "    \"\"\"\n",
    "    frames = list_files(imagepath, 'jpg', reverse_sort = reverse)\n",
    "    img_array = []\n",
    "    print(\"Reading frames.\")\n",
    "    for index, frame in enumerate(frames):\n",
    "        img = cv2.imread(frame)\n",
    "        img_array.append(img)\n",
    "    h,w,l = img_array[0].shape\n",
    "    shape = (w,h)\n",
    "    out = cv2.VideoWriter(imagepath+'aligned.mp4',\n",
    "                          cv2.VideoWriter_fourcc(*'mp4v'), 25, shape)\n",
    "    print(\"Writing frames to video.\")\n",
    "    for i in range(len(img_array)):\n",
    "        out.write(img_array[i])\n",
    "    out.release()\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CNN model.\n",
      "Loading trained model weights.\n",
      "Weights for Affine model loaded.\n",
      "Weights for Thin-Plate Spline model loaded.\n",
      "Returning model(s).\n"
     ]
    }
   ],
   "source": [
    "path_affine =  './trained_models/wormbrain_simplerfr/affine_wormbrain_simplerfr.pth.tar'\n",
    "path_tps = './trained_models/wormbrain_simplerfr/tps_wormbrain_simplerfr.pth.tar'\n",
    "model_affine, model_tps = load_model(path_affine, 'wormbrain_1','simpler',\n",
    "                                     path_tps, 'wormbrain_1','simpler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### Aligning ###\n",
      "Done aligning.\n",
      "Reading frames.\n",
      "Writing frames to video.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#on \"test\" data\n",
    "batch_align_to_previous('../worm_data/video/frames/',model_affine, model_tps, iterations = 1)\n",
    "save_video('../worm_data/video/frames/alignedprevious/iteration_0/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
